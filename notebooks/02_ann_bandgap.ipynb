{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Summary: Band Gap Prediction (Materials Project) — ANN vs Tree Ensembles\n",
    "> **Razieh Morad**\n",
    "\n",
    "- **Problem:** Predict PBE band gap (eV) for stable binary semiconductors.\n",
    "- **Data:** Materials Project (MP API), one representative per formula (N = …).\n",
    "- **Approach:** \n",
    ">- Composition featurization: matminer \n",
    ">- Artificial Neural Network (ANN) vs. RF/XGB/GBR \n",
    ">- Split: IID (stratified by binned band gap)\n",
    "- **Metrics:** MAE / RMSE / R² on **test**; parity plots; XGB feature importance.\n",
    "- **Takeaway:** _Fill after results_: e.g., “XGB wins on IID (MAE A→B, R² C→D vs. Ridge/ANN). ANN competitive, but needs more data/regularization.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 0) Imports & Config\n",
    "# ======================\n",
    "import os, math, json, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mp_api.client import MPRester\n",
    "from pymatgen.core import Composition\n",
    "\n",
    "from matminer.featurizers.base import MultipleFeaturizer\n",
    "from matminer.featurizers.composition import (\n",
    "    ElementProperty, OxidationStates, AtomicOrbitals, BandCenter\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# Repro / paths\n",
    "RNG_SEED = 42\n",
    "np.random.seed(RNG_SEED)\n",
    "ROOT = Path.cwd()\n",
    "FIGS = ROOT.parent / \"reports\" / \"figures\" if (ROOT.name == \"notebooks\") else ROOT / \"reports\" / \"figures\"\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Working dir:\", ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Load datasets of binary semiconductors from Materials Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. SET API KEY\n",
    "# ======================\n",
    "api_key = \"VzNVmBMga05iZbWxZz3XFrV74z9ih3jE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fields = [\n",
    "    \"material_id\",\"formula_pretty\",\"elements\",\"nsites\",\"nelements\",\"density\",\"volume\",\n",
    "    \"energy_per_atom\",\"formation_energy_per_atom\",\"energy_above_hull\",\"band_gap\",\"is_gap_direct\",\n",
    "    \"total_magnetization\",\"bulk_modulus\",\"shear_modulus\",\"homogeneous_poisson\",\"efermi\",\n",
    "    \"universal_anisotropy\",\"n\",\"e_total\"\n",
    "]\n",
    "\n",
    "print(\"############  Fetching stable binary semiconductors… \")\n",
    "with MPRester(api_key) as mpr:\n",
    "    docs = mpr.materials.summary.search(\n",
    "        num_elements=[2, 2],\n",
    "        energy_above_hull=(0, 0.10),\n",
    "        is_metal=False,\n",
    "        band_gap=(0.01, None),\n",
    "        chunk_size=100,\n",
    "        num_chunks=10,              # ↓ reduce to 3–5 if it's too slow\n",
    "        fields=valid_fields,\n",
    "    )\n",
    "\n",
    "rows = []\n",
    "for d in docs:\n",
    "    rows.append({\n",
    "        \"material_id\": d.material_id,\n",
    "        \"formula\": d.formula_pretty,\n",
    "        \"elements\": tuple(d.elements) if getattr(d, \"elements\", None) else None,\n",
    "        \"nsites\": d.nsites,\n",
    "        \"nelements\": d.nelements,\n",
    "        \"density\": d.density,\n",
    "        \"volume\": d.volume,\n",
    "        \"energy_per_atom\": d.energy_per_atom,\n",
    "        \"formation_energy_per_atom\": d.formation_energy_per_atom,\n",
    "        \"energy_above_hull\": d.energy_above_hull,\n",
    "        \"band_gap\": d.band_gap,\n",
    "        \"is_gap_direct\": int(d.is_gap_direct) if d.is_gap_direct is not None else 0,\n",
    "        \"total_magnetization\": d.total_magnetization,\n",
    "        \"bulk_modulus\": d.bulk_modulus,\n",
    "        \"shear_modulus\": d.shear_modulus,\n",
    "        \"poisson_ratio\": d.homogeneous_poisson,\n",
    "        \"fermi_energy\": d.efermi,\n",
    "        \"elastic_anisotropy\": d.universal_anisotropy,\n",
    "        \"refractive_index\": d.n,\n",
    "        \"dielectric_constant\": d.e_total,\n",
    "    })\n",
    "\n",
    "df_raw = pd.DataFrame(rows)\n",
    "print(\"Fetched:\", df_raw.shape)\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. QA & deduplicate\n",
    "\n",
    "- Keep one representative per formula (lowest `E_hull`, then smallest volume).  \n",
    "- Drop broken rows / negative band gaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = (\n",
    "    df_raw.sort_values([\"energy_above_hull\", \"volume\"], ascending=[True, True])\n",
    "          .drop_duplicates(\"formula\", keep=\"first\")\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_raw = df_raw.dropna(subset=[\"band_gap\"]).query(\"band_gap >= 0\")\n",
    "print(\"After dedup & QA:\", df_raw.shape)\n",
    "df_raw.describe(include=\"all\").T.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Featurize (composition-only; fast & solid)\n",
    "\n",
    "Using matminer to compute composition descriptors: **Magpie**, oxidation states, atomic orbitals, and band center.  \n",
    "Keeping numeric columns for ML and carry a **meta** table (formula, elements) for slicing/plots later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composition objects\n",
    "df_feat = df_raw.copy()\n",
    "df_feat[\"composition\"] = df_feat[\"formula\"].apply(Composition)\n",
    "\n",
    "featurizer = MultipleFeaturizer([\n",
    "    ElementProperty.from_preset(\"magpie\"),\n",
    "    OxidationStates(),\n",
    "    AtomicOrbitals(),\n",
    "    BandCenter(),\n",
    "])\n",
    "\n",
    "# Compute features in-place (ignore per-row errors)\n",
    "df_feat = featurizer.featurize_dataframe(df_feat, col_id=\"composition\", ignore_errors=True)\n",
    "\n",
    "# Keep meta for later slicing\n",
    "meta = df_feat[[\"formula\",\"elements\"]].copy()\n",
    "\n",
    "# Keep numeric features only (drop target later)\n",
    "# meta holds non-numeric context (formula, elements) for plots/slicing/OOD masks.\n",
    "df_num = df_feat.select_dtypes(include=[np.number]).copy()\n",
    "df_num = df_num.dropna(subset=[\"band_gap\"]).reset_index(drop=True)\n",
    "meta = meta.loc[df_num.index].reset_index(drop=True)\n",
    "\n",
    "print(\"Numeric feature table:\", df_num.shape)\n",
    "df_num.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Split (IID) with stratification, and preprocessing per model family\n",
    "\n",
    "- **Trees**: impute missing values (no scaling needed).  \n",
    "- **ANN**: impute + scale X, and scale **y** for stable training.  \n",
    "- We stratify the split by **binned** band gap to match target distribution in train/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate X and y\n",
    "X = df_num.drop(columns=[\"band_gap\"])\n",
    "y = df_num[\"band_gap\"].values.astype(np.float32)\n",
    "\n",
    "# Use indices to keep alignment with meta\n",
    "idx = np.arange(len(X))\n",
    "bins = pd.qcut(y, q=10, duplicates=\"drop\")\n",
    "\n",
    "idx_tr, idx_te = train_test_split(\n",
    "    idx, test_size=0.2, random_state=RNG_SEED, stratify=bins\n",
    ")\n",
    "\n",
    "X_train = X.iloc[idx_tr]; X_test  = X.iloc[idx_te]\n",
    "y_train = y[idx_tr];      y_test  = y[idx_te]\n",
    "meta_te = meta.iloc[idx_te].reset_index(drop=True)\n",
    "\n",
    "# Preprocessing\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train_tree = imp.fit_transform(X_train)\n",
    "X_test_tree  = imp.transform(X_test)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_ann = scaler_X.fit_transform(X_train_tree)   # use imputed numeric arrays\n",
    "X_test_ann  = scaler_X.transform(X_test_tree)\n",
    "\n",
    "print(\"Train shapes — tree:\", X_train_tree.shape, \" | ann:\", X_train_ann.shape)\n",
    "print(f\"y range (eV): {y.min():.2f}–{y.max():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def chk(name, arr):\n",
    "    arr = np.asarray(arr)\n",
    "    print(f\"{name:12s} shape={arr.shape}  finite={np.isfinite(arr).all()}  \"\n",
    "          f\"n_nan={np.isnan(arr).sum()}  n_inf={np.isinf(arr).sum()}\")\n",
    "\n",
    "chk(\"X_train_tree\", X_train_tree)\n",
    "chk(\"X_test_tree\",  X_test_tree)\n",
    "chk(\"X_train_ann\",  X_train_ann)\n",
    "chk(\"X_test_ann\",   X_test_ann)\n",
    "\n",
    "print(\"y_train stats (eV): mean=%.3f std=%.3f\" % (y_train.mean(), y_train.std()))\n",
    "print(\"y_test  stats (eV): mean=%.3f std=%.3f\" % (y_test.mean(),  y_test.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def chk(name, arr):\n",
    "    print(f\"{name}: shape={arr.shape}  finite={np.isfinite(arr).all()}  \"\n",
    "          f\"n_nan={np.isnan(arr).sum()}  n_inf={np.isinf(arr).sum()}\")\n",
    "\n",
    "chk(\"X_train_ann\", X_train_ann)\n",
    "chk(\"X_test_ann\",  X_test_ann)\n",
    "\n",
    "# If using StandardScaler, inspect its scales (std dev per feature)\n",
    "try:\n",
    "    print(\"min/std/median scaler_X.scale_:\", np.min(scaler_X.scale_), np.max(scaler_X.scale_), np.median(scaler_X.scale_))\n",
    "except Exception as e:\n",
    "    print(\"No scaler_X.scale_ yet:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Impute numeric features\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = imp.fit_transform(X_train)\n",
    "X_test_imp  = imp.transform(X_test)\n",
    "\n",
    "# Drop zero-variance columns (prevents 1/0 in StandardScaler)\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "X_train_vt = vt.fit_transform(X_train_imp)\n",
    "X_test_vt  = vt.transform(X_test_imp)\n",
    "\n",
    "# Scale for ANN\n",
    "scaler_X = StandardScaler()\n",
    "X_train_ann = scaler_X.fit_transform(X_train_vt)\n",
    "X_test_ann  = scaler_X.transform(X_test_vt)\n",
    "\n",
    "# Quick guards\n",
    "assert np.isfinite(X_train_ann).all() and np.isfinite(X_test_ann).all(), \"Non-finite values after scaling!\"\n",
    "print(\"✅ preprocessing OK —\",\n",
    "      \"X_train_ann\", X_train_ann.shape, \"| X_test_ann\", X_test_ann.shape,\n",
    "      \"| min scale\", scaler_X.scale_.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 5. ANN model (PyTorch) + K-fold CV (with **per-fold** y-scaler)\n",
    "\n",
    "We scale **y** inside each fold to avoid leakage. Final test predictions are inverse-transformed with the best fold’s scaler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 5) ANN (PyTorch) + K-fold CV with PER-FOLD y-scaler\n",
    "# ======================\n",
    "# This cell trains an MLP (ANN) with:\n",
    "# - input scaling already applied to X (X_train_ann / X_test_ann)\n",
    "# - target scaling applied PER FOLD (no leakage)\n",
    "# - early stopping via patience on validation loss\n",
    "# - ReduceLROnPlateau scheduler for smoother convergence\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "RNG_SEED = 42\n",
    "torch.manual_seed(RNG_SEED)\n",
    "np.random.seed(RNG_SEED)\n",
    "\n",
    "# ----------------------\n",
    "# ANN architecture  nn.Dropout(0.15)\n",
    "# ----------------------\n",
    "class BandGapANN(nn.Module):\n",
    "    def __init__(self, d_in: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, 256), nn.ReLU(), nn.Dropout(0.05),\n",
    "            nn.Linear(256, 128),  nn.ReLU(), nn.Dropout(0.05),\n",
    "            nn.Linear(128,  64),  nn.ReLU(), nn.Dropout(0.05),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------\n",
    "# One training run (given a train/val split, with y already scaled)\n",
    "# ----------------------\n",
    "def train_ann_epoched(Xtr, ytr_scaled, Xva, yva_scaled, n_epochs=100, batch_size=32, lr=5e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # numpy -> torch\n",
    "    Xtr_t = torch.tensor(Xtr, dtype=torch.float32).to(device)\n",
    "    ytr_t = torch.tensor(ytr_scaled, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    Xva_t = torch.tensor(Xva, dtype=torch.float32).to(device)\n",
    "    yva_t = torch.tensor(yva_scaled, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = BandGapANN(Xtr.shape[1]).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    crit = nn.MSELoss()\n",
    "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=5)\n",
    "\n",
    "    best_state, best_val, patience, max_patience = model.state_dict(), float(\"inf\"), 0, 10\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in loader:\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(xb), yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "            running += loss.item() * xb.size(0)\n",
    "        train_loss = running / len(loader.dataset)\n",
    "\n",
    "        # --- validate ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = crit(model(Xva_t), yva_t).item()\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        sched.step(val_loss)\n",
    "\n",
    "        # --- early stopping bookkeeping ---\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= max_patience:\n",
    "                break\n",
    "\n",
    "    # restore best weights\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# ----------------------\n",
    "# K-fold wrapper with PER-FOLD y-scaler (no leakage)\n",
    "# ----------------------\n",
    "def kfold_ann_with_perfold_scaler(X_ann, y_ev, n_splits=5, n_epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    X_ann : numpy array (imputed + scaled features for ANN)\n",
    "    y_ev  : numpy array of band gaps in eV (UNSCALED)\n",
    "    Returns: list of (model, y_scaler) for each fold, and a DataFrame with fold metrics.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RNG_SEED)\n",
    "    fold_rows, fold_models = [], []\n",
    "\n",
    "    for i, (tr, va) in enumerate(kf.split(X_ann), start=1):\n",
    "        # ---- PER-FOLD y-scaler: fit on y_train_fold ONLY (prevents leakage)\n",
    "        y_scaler = StandardScaler().fit(y_ev[tr].reshape(-1, 1))\n",
    "        y_tr_scaled = y_scaler.transform(y_ev[tr].reshape(-1, 1)).ravel()\n",
    "        y_va_scaled = y_scaler.transform(y_ev[va].reshape(-1, 1)).ravel()\n",
    "\n",
    "        # ---- Train one ANN on this fold\n",
    "        model, tr_loss, va_loss = train_ann_epoched(\n",
    "            X_ann[tr], y_tr_scaled,\n",
    "            X_ann[va], y_va_scaled,\n",
    "            n_epochs=n_epochs, batch_size=batch_size, lr=1e-3\n",
    "        )\n",
    "\n",
    "        fold_models.append((model, y_scaler))\n",
    "\n",
    "        # ---- Predict on validation fold, inverse-transform back to eV for honest metrics\n",
    "        with torch.no_grad():\n",
    "            preds_scaled = model(torch.tensor(X_ann[va], dtype=torch.float32)).cpu().numpy().ravel()\n",
    "        preds_ev = y_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).ravel()\n",
    "        truth_ev = y_ev[va]\n",
    "\n",
    "        mae  = mean_absolute_error(truth_ev, preds_ev)\n",
    "        rmse = math.sqrt(mean_squared_error(truth_ev, preds_ev))\n",
    "        r2   = r2_score(truth_ev, preds_ev)\n",
    "        fold_rows.append(dict(fold=i, MAE=mae, RMSE=rmse, R2=r2))\n",
    "\n",
    "        # (optional) quick curve\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plt.plot(tr_loss, label=\"train\"); plt.plot(va_loss, label=\"val\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f\"ANN training curves (fold {i})\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    cvdf = pd.DataFrame(fold_rows)\n",
    "    print(cvdf)\n",
    "    print(f\"ANN CV — mean R²: {cvdf.R2.mean():.3f} ± {cvdf.R2.std():.3f}\")\n",
    "    return fold_models, cvdf\n",
    "\n",
    "# ----------------------\n",
    "# Run K-fold on your training split, then evaluate once on TEST\n",
    "# ----------------------\n",
    "print(\" Training ANN with K-fold CV on training split…\")\n",
    "ann_models, ann_cv = kfold_ann_with_perfold_scaler(\n",
    "    X_train_ann,  # features for ANN (already imputed+scaled)\n",
    "    y_train,      # target in eV (UNSCALED here)\n",
    "    n_splits=5, n_epochs=100, batch_size=32\n",
    ")\n",
    "\n",
    "# Pick best fold by R², use ITS y-scaler to map test preds back to eV\n",
    "best_idx = ann_cv[\"R2\"].idxmax()\n",
    "best_ann, best_y_scaler = ann_models[best_idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_test_scaled = best_ann(torch.tensor(X_test_ann, dtype=torch.float32)).cpu().numpy().ravel()\n",
    "y_pred_test_ev = best_y_scaler.inverse_transform(y_pred_test_scaled.reshape(-1,1)).ravel()\n",
    "\n",
    "mae  = mean_absolute_error(y_test, y_pred_test_ev)\n",
    "rmse = math.sqrt(mean_squared_error(y_test, y_pred_test_ev))\n",
    "r2   = r2_score(y_test, y_pred_test_ev)\n",
    "print(f\"ANN (TEST) — MAE {mae:.3f} | RMSE {rmse:.3f} | R² {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity(y_true, y_pred, title):\n",
    "    import numpy as np, matplotlib.pyplot as plt, os\n",
    "    os.makedirs(\"reports/figures\", exist_ok=True)\n",
    "    lim = (0, max(np.max(y_true), np.max(y_pred)))\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(y_true, y_pred, s=6, alpha=0.5)\n",
    "    plt.plot(lim, lim, \"--\"); plt.xlim(lim); plt.ylim(lim)\n",
    "    plt.xlabel(\"True band gap (eV)\"); plt.ylabel(\"Predicted (eV)\"); plt.title(title)\n",
    "    plt.tight_layout(); plt.savefig(\"reports/figures/parity_ann.png\", dpi=160); plt.show()\n",
    "\n",
    "parity(y_test, y_pred_test_ev, \"ANN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core import Composition\n",
    "HOLDOUT = {\"Bi\",\"Te\"}\n",
    "\n",
    "def has_holdout(formula): \n",
    "    try: return any(el.symbol in HOLDOUT for el in Composition(formula).elements)\n",
    "    except: return False\n",
    "\n",
    "mask_ood = meta[\"formula\"].apply(has_holdout).values\n",
    "X_ood = X.loc[mask_ood]\n",
    "\n",
    "# transform with the SAME train preprocessors used for ANN\n",
    "X_ood_imp = imp.transform(X_ood)\n",
    "X_ood_vt  = vt.transform(X_ood_imp)          # if you used VarianceThreshold; else skip this line\n",
    "X_ood_ann = scaler_X.transform(X_ood_vt if 'vt' in globals() else X_ood_imp)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_ood_s = best_ann(torch.tensor(X_ood_ann, dtype=torch.float32)).cpu().numpy().ravel()\n",
    "y_ood_pred = best_y_scaler.inverse_transform(pred_ood_s.reshape(-1,1)).ravel()\n",
    "y_ood_true = y[mask_ood]\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import math\n",
    "print(f\"ANN OOD {sorted(HOLDOUT)} — MAE {mean_absolute_error(y_ood_true, y_ood_pred):.3f} | \"\n",
    "      f\"RMSE {math.sqrt(mean_squared_error(y_ood_true, y_ood_pred)):.3f} | \"\n",
    "      f\"R² {r2_score(y_ood_true, y_ood_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 6. Classical models (trees) on imputed features\n",
    "\n",
    "- Trees don’t need scaling; we use the imputed numeric arrays.  \n",
    "- We compute comparable test metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Trees preprocessing ---\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train_tree = imp.fit_transform(X_train)\n",
    "X_test_tree  = imp.transform(X_test)\n",
    "\n",
    "# Optional but recommended (prevents 1/0 std issues later)\n",
    "vt = VarianceThreshold(threshold=0.0)\n",
    "X_train_tree = vt.fit_transform(X_train_tree)\n",
    "X_test_tree  = vt.transform(X_test_tree)\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=300, max_depth=12, random_state=0, n_jobs=-1),\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=400, learning_rate=0.06, max_depth=8,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "        tree_method=\"hist\", random_state=0, n_jobs=-1\n",
    "    ),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(\n",
    "        n_estimators=400, learning_rate=0.06, max_depth=6, random_state=0\n",
    "    ),\n",
    "}\n",
    "\n",
    "results, preds = [], {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_tree, y_train)          # <-- eV target\n",
    "    yp = model.predict(X_test_tree)           # <-- on matching tree features\n",
    "    preds[name] = yp\n",
    "    mae  = mean_absolute_error(y_test, yp)\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, yp))\n",
    "    r2   = r2_score(y_test, yp)\n",
    "    results.append(dict(Model=name, MAE=mae, RMSE=rmse, R2=r2))\n",
    "\n",
    "# Add ANN (your existing y_pred_ann in eV)\n",
    "results.append(dict(Model=\"ANN\",\n",
    "                    MAE=mean_absolute_error(y_test, y_pred_ann),\n",
    "                    RMSE=math.sqrt(mean_squared_error(y_test, y_pred_ann)),\n",
    "                    R2=r2_score(y_test, y_pred_ann)))\n",
    "results_df = pd.DataFrame(results).sort_values(\"R2\", ascending=False).reset_index(drop=True)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 7. Parity plots (saved) & XGB feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ensure output folder exists (use your existing FIGS if defined)\n",
    "try:\n",
    "    FIGS\n",
    "except NameError:\n",
    "    FIGS = Path(\"reports/figures\")\n",
    "FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def parity_grid(y_true, pred_dict, layout=\"2x2\", fname=\"parity_grid.png\"):\n",
    "    \"\"\"\n",
    "    layout: \"2x2\" for a 2-by-2 grid, or \"row\" for all side-by-side in one row.\n",
    "    \"\"\"\n",
    "    names = list(pred_dict.keys())\n",
    "    preds = [pred_dict[k] for k in names]\n",
    "\n",
    "    # consistent axes across all panels\n",
    "    max_val = max(float(np.max(y_true)), max(float(np.max(p)) for p in preds))\n",
    "    lim = (0, max_val)\n",
    "\n",
    "    if layout == \"row\":\n",
    "        nrows, ncols = 1, len(names)\n",
    "    else:  # \"2x2\" (default) or auto for up to 4 models\n",
    "        ncols = 2\n",
    "        nrows = math.ceil(len(names) / ncols)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 5*nrows), squeeze=False)\n",
    "    axes_flat = axes.ravel()\n",
    "\n",
    "    for i, (name, yp) in enumerate(zip(names, preds)):\n",
    "        ax = axes_flat[i]\n",
    "        ax.scatter(y_true, yp, s=6, alpha=0.5)\n",
    "        ax.plot(lim, lim, \"--\")\n",
    "        ax.set_xlim(lim); ax.set_ylim(lim)\n",
    "        ax.set_xlabel(\"True band gap (eV)\")\n",
    "        ax.set_ylabel(\"Predicted (eV)\")\n",
    "        ax.set_title(name)\n",
    "\n",
    "    # hide any unused subplots\n",
    "    for j in range(i+1, len(axes_flat)):\n",
    "        axes_flat[j].axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out = FIGS / fname\n",
    "    fig.savefig(out, dpi=160)\n",
    "    plt.show()\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# Usage:\n",
    "# 2x2 grid\n",
    "#parity_grid(y_test, preds, layout=\"2x2\", fname=\"parity_grid_2x2.png\")\n",
    "\n",
    "# side-by-side in one row\n",
    "parity_grid(y_test, preds, layout=\"row\", fname=\"parity_grid_row.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB feature importance\n",
    "xgb_model = models[\"XGBoost\"]   # already fitted\n",
    "imps = xgb_model.feature_importances_\n",
    "feat_names = X.columns.tolist()   # corresponds to columns used before impute\n",
    "\n",
    "top = np.argsort(imps)[::-1][:15]\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(range(len(top)), imps[top])\n",
    "plt.xticks(range(len(top)), [feat_names[i] for i in top], rotation=60, ha=\"right\")\n",
    "plt.title(\"Top 15 Feature Importances (XGBoost)\")\n",
    "plt.tight_layout()\n",
    "out = FIGS / \"xgb_feature_importance.png\"\n",
    "plt.savefig(out, dpi=160); plt.show()\n",
    "print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 8. (Optional) OOD hold-out by elements (chemistry shift)\n",
    "\n",
    "Train on compounds **without** selected elements and test on those **with** them.  \n",
    "This simulates a domain shift (e.g., cold-start chemistries).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"type(y):\", type(y), \"shape:\", getattr(y, \"shape\", None))\n",
    "X_full = df_num.drop(columns=[\"band_gap\"]).copy()\n",
    "y_full = df_num[\"band_gap\"].values.astype(np.float32)\n",
    "meta_full = meta.copy()  # must correspond 1:1 with df_num\n",
    "\n",
    "assert len(X_full) == len(y_full) == len(meta_full), \"lengths must match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymatgen.core import Composition\n",
    "\n",
    "HOLDOUT = {\"Bi\",\"Te\"}  # adjust if too small/large\n",
    "\n",
    "def has_holdout(formula: str) -> bool:\n",
    "    try:\n",
    "        return any(el.symbol in HOLDOUT for el in Composition(formula).elements)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "mask_ood = meta_full[\"formula\"].apply(has_holdout).values  # length == len(X_full)\n",
    "\n",
    "# In-domain (train) vs OOD (test)\n",
    "X_tr_ood, y_tr_ood = X_full.loc[~mask_ood], y_full[~mask_ood]\n",
    "X_te_ood, y_te_ood = X_full.loc[ mask_ood], y_full[ mask_ood]\n",
    "\n",
    "print(\"OOD sizes:\", X_tr_ood.shape, X_te_ood.shape)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import math\n",
    "\n",
    "# Impute & drop zero-variance cols using ONLY in-domain data\n",
    "imp_ood = SimpleImputer(strategy=\"median\").fit(X_tr_ood)\n",
    "X_tr_imp = imp_ood.transform(X_tr_ood)\n",
    "X_te_imp = imp_ood.transform(X_te_ood)\n",
    "\n",
    "vt_ood = VarianceThreshold(0.0).fit(X_tr_imp)\n",
    "X_tr_vt = vt_ood.transform(X_tr_imp)\n",
    "X_te_vt = vt_ood.transform(X_te_imp)\n",
    "\n",
    "# Train on in-domain, test on OOD\n",
    "xgb_ood = XGBRegressor(\n",
    "    n_estimators=400, learning_rate=0.06, max_depth=8,\n",
    "    subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "    tree_method=\"hist\", random_state=0, n_jobs=-1\n",
    ").fit(X_tr_vt, y_tr_ood)\n",
    "\n",
    "y_pred_ood = xgb_ood.predict(X_te_vt)\n",
    "mae  = mean_absolute_error(y_te_ood, y_pred_ood)\n",
    "rmse = math.sqrt(mean_squared_error(y_te_ood, y_pred_ood))\n",
    "r2   = r2_score(y_te_ood, y_pred_ood)\n",
    "print(f\"XGB OOD {sorted(HOLDOUT)} — MAE {mae:.3f} | RMSE {rmse:.3f} | R² {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 9. (Optional) Save artifacts\n",
    "\n",
    "Only if you want a small inference demo later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(models[\"XGBoost\"], \"models/xgb_bandgap.joblib\")\n",
    "np.savez(\"models/ann_scalers.npz\",\n",
    "         imp_statistics=imp.statistics_,   # imputer stats\n",
    "         scaler_X_mean=scaler_X.mean_, scaler_X_scale=scaler_X.scale_)\n",
    "print(\"Saved models/… (XGB + scalers)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thermo-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
